{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1oaA_sArFhrDYnvSMbsDhGBKlqmd5e8cs","timestamp":1683036986898}],"authorship_tag":"ABX9TyM8TxuwqyzoRR0wA7y8j1n0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Access Data"],"metadata":{"id":"EX-csGyt9dLm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXoy5CgZ2tHV"},"outputs":[],"source":["# Access Google Drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["Import Data"],"metadata":{"id":"sTCAzZMCBiOe"}},{"cell_type":"code","source":["# Required Imports\n","import pandas as pd\n","\n","\n","# Steep Downhill -10%\n","TP1_DW_10 = pd.read_csv()\n","TP2_DW_10 = pd.read_csv()\n","TP3_DW_10 = pd.read_csv()\n","TP4_DW_10 = pd.read_csv()\n","TP5_DW_10 = pd.read_csv()\n","TP6_DW_10 = pd.read_csv()\n","TP7_DW_10 = pd.read_csv()\n","\n","\n","# Shallow Downhill -5% \n","TP1_DW_5 = pd.read_csv()\n","TP2_DW_5 = pd.read_csv()\n","TP3_DW_5 = pd.read_csv()\n","TP4_DW_5 = pd.read_csv()\n","TP5_DW_5 = pd.read_csv()\n","TP6_DW_5 = pd.read_csv()\n","TP7_DW_5 = pd.read_csv()\n","\n","\n","# Level Ground 0% \n","TP1_LG = pd.read_csv()\n","TP2_LG = pd.read_csv()\n","TP3_LG = pd.read_csv()\n","TP4_LG = pd.read_csv()\n","TP5_LG = pd.read_csv()\n","TP6_LG = pd.read_csv()\n","TP7_LG = pd.read_csv()\n","\n","\n","# Shallow Uphill 5%\n","TP1_UW_5 = pd.read_csv()\n","TP2_UW_5 = pd.read_csv()\n","TP3_UW_5 = pd.read_csv()\n","TP4_UW_5 = pd.read_csv()\n","TP5_UW_5 = pd.read_csv()\n","TP6_UW_5 = pd.read_csv()\n","TP7_UW_5 = pd.read_csv()\n","\n","\n","# Steep Uphill 10%\n","TP1_UW_10 = pd.read_csv()\n","TP2_UW_10 = pd.read_csv()\n","TP3_UW_10 = pd.read_csv()\n","TP4_UW_10 = pd.read_csv()\n","TP5_UW_10 = pd.read_csv()\n","TP6_UW_10 = pd.read_csv()\n","TP7_UW_10 = pd.read_csv()"],"metadata":{"id":"uvQHgHtzBjhW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Data Preprocessing"],"metadata":{"id":"NE6HvqzO9hbZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DrmM_BEcPSw0"},"outputs":[],"source":["# LOOCV generation of training, validation and testing datasets\n","\n","# Required Imports\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","import numpy as np\n","import tensorflow as tf\n","\n","\n","# Concatenate Modes for Training\n","TR_DW_10 = pd.concat([TP1_DW_10, TP2_DW_10, TP3_DW_10, TP4_DW_10, TP5_DW_10])\n","TR_DW_5 = pd.concat([TP1_DW_5, TP2_DW_5, TP3_DW_5, TP4_DW_5, TP5_DW_5])\n","TR_LG = pd.concat([TP1_LG, TP2_LG, TP3_LG, TP4_LG, TP5_LG])\n","TR_UW_5 = pd.concat([TP1_UW_5, TP2_UW_5, TP3_UW_5, TP4_UW_5, TP5_UW_5])\n","TR_UW_10 = pd.concat([TP1_UW_10, TP2_UW_10, TP3_UW_10, TP4_UW_10, TP5_UW_10])\n","\n","\n","# Seperate Dataset for Validation\n","V_DW_10 = TP6_DW_10\n","V_DW_5 = TP6_DW_5\n","V_LG = TP6_LG\n","V_UW_5 = TP6_UW_5\n","V_UW_10 = TP6_UW_10\n","\n","\n","# Seperate Dataset for Testing\n","TE_DW_10 = TP7_DW_10\n","TE_DW_5 = TP7_DW_5\n","TE_LG = TP7_LG\n","TE_UW_5 = TP7_UW_5\n","TE_UW_10 = TP7_UW_10\n","\n","\n","# Add Labels to Training Data \n","Mode = [0, 0, 0]\n","TR_DW_10['Mode'], V_DW_10['Mode'], TE_DW_10['Mode'] = Mode\n","Mode = [1, 1, 1]\n","TR_DW_5['Mode'], V_DW_5['Mode'], TE_DW_5['Mode'] = Mode\n","Mode = [2, 2, 2]\n","TR_LG['Mode'], V_LG['Mode'], TE_LG['Mode'] = Mode\n","Mode = [3, 3, 3]\n","TR_UW_5['Mode'], V_UW_5['Mode'], TE_UW_5['Mode'] = Mode\n","Mode = [4, 4, 4]\n","TR_UW_10['Mode'], V_UW_10['Mode'], TE_UW_10['Mode'] = Mode\n","\n","\n","# Determine minimum length to balance classes\n","TR_Length = min(len(TR_DW_10), len(TR_DW_5), len(TR_LG), len(TR_UW_5), len(TR_UW_10))\n","V_Length = min(len(V_DW_10), len(V_DW_5), len(V_LG), len(V_UW_5), len(V_UW_10))\n","TE_Length = min(len(TE_DW_10), len(TE_DW_5), len(TE_LG), len(TE_UW_5), len(TE_UW_10))\n","\n","\n","# Concatenate all data into single dataframe and reset indexing\n","TR_Data = pd.concat([TR_DW_10[0:TR_Length], TR_DW_5[0:TR_Length], TR_LG[0:TR_Length], TR_UW_5[0:TR_Length], TR_UW_10[0:TR_Length]], ignore_index=True)\n","V_Data = pd.concat([V_DW_10[0:V_Length], V_DW_5[0:V_Length], V_LG[0:V_Length], V_UW_5[0:V_Length], V_UW_10[0:V_Length]], ignore_index=True)\n","TE_Data = pd.concat([TE_DW_10[0:TE_Length], TE_DW_5[0:TE_Length], TE_LG[0:TE_Length], TE_UW_5[0:TE_Length], TE_UW_10[0:TE_Length]], ignore_index=True)\n","\n","\n","# Set index to equal the Time column\n","TR_Data = TR_Data.set_index(\"Time(s)\")\n","V_Data = V_Data.set_index(\"Time(s)\")\n","TE_Data = TE_Data.set_index(\"Time(s)\")\n","\n","\n","# Split data into inputs and labels\n","TR_X = TR_Data.iloc[:, :-1].values\n","TR_Y = TR_Data.iloc[:, -1:].values\n","V_X = V_Data.iloc[:, :-1].values\n","V_Y = V_Data.iloc[:, -1:].values\n","TE_X = TE_Data.iloc[:, :-1].values\n","TE_Y = TE_Data.iloc[:, -1:].values\n","\n","\n","# Scale data to range [-1, 1]\n","scaler = MinMaxScaler(feature_range=(-1, 1))\n","TR_X = scaler.fit_transform(TR_X)\n","V_X = scaler.fit_transform(V_X)\n","TE_X = scaler.fit_transform(TE_X)\n","\n","\n","# Reshape labels to 2D array\n","TR_Y = np.reshape(TR_Y, (-1, 1))\n","V_Y = np.reshape(V_Y, (-1, 1))\n","TE_Y = np.reshape(TE_Y, (-1, 1))\n","\n","\n","# Define Window and Batch Size and Number of Features of Data\n","window_size = 200\n","batch_size = 128\n","num_features = 6\n","sequence_stride = 50\n","\n","# Create sliding window training dataset\n","TR_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n","    data=TR_X,\n","    targets=TR_Y,\n","    sequence_length=window_size,\n","    sequence_stride=sequence_stride,\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","\n","\n","# Create sliding window validation dataset\n","V_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n","    data=V_X,\n","    targets=V_Y,\n","    sequence_length=window_size,\n","    sequence_stride=sequence_stride,\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","\n","\n","# Create sliding window testing dataset\n","TE_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n","    data=TE_X,\n","    targets=TE_Y,\n","    sequence_length=window_size,\n","    sequence_stride=sequence_stride,\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n"]},{"cell_type":"markdown","source":["Train Model"],"metadata":{"id":"8YrGbFyXy1PK"}},{"cell_type":"code","source":["# Required Imports\n","import numpy as np\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.layers import Input, LSTM, Dropout, RepeatVector, TimeDistributed, Dense, Flatten\n","from keras import optimizers\n","\n","# Define Model\n","model = Sequential()\n","model.add(LSTM(200, activation='tanh', input_shape=(n_steps_in, n_features)))\n","model.add(RepeatVector(n_steps_out))\n","model.add(LSTM(200, activation='tanh', return_sequences=True))\n","model.add(TimeDistributed(Dense(100, activation='tanh')))\n","model.add(TimeDistributed(Dense(1)))\n","model.compile(loss='mse', optimizer='adam')\n","\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","\n","# Train model\n","history = model.fit(TR_dataset, epochs=50, validation_data=V_dataset)"],"metadata":{"id":"p3C1cEvN77zO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test Model"],"metadata":{"id":"N3sTqLZS0FfH"}},{"cell_type":"code","source":["# Required Import\n","import time\n","\n","# Initialize variables to keep track of prediction results\n","total_predictions = 0\n","correct_predictions = 0\n","\n","# Make predictions in real-time\n","predictions = []\n","start_time = time.time()\n","\n","for i, (inputs, labels) in enumerate(TE_dataset):\n","    # Measure computation time for prediction\n","    batch_start_time = time.time()\n","    # Make prediction\n","    y_pred = model.predict(inputs)\n","    # Extract predicted label from integer encoding\n","    label_pred = np.argmax(y_pred, axis=-1)\n","    # Append predicted label to list\n","    predictions.append(label_pred)\n","    # Compare with true label and update counters\n","    for pred, true in zip(label_pred, labels):\n","        total_predictions += 1\n","        if pred == true:\n","            correct_predictions += 1\n","            prediction_result = \"Correct\"\n","        else:\n","            prediction_result = \"Incorrect\"\n","        # Print prediction result and computation time for prediction\n","        batch_end_time = time.time()\n","        batch_time = batch_end_time - batch_start_time\n","        print(f\"Prediction: {pred}, True label: {true}, Result: {prediction_result}, Computation time: {batch_time:.4f} seconds\")\n","\n","end_time = time.time()\n","print(f\"Total computation time: {end_time - start_time:.2f} seconds\")\n","average_time = (end_time - start_time) / total_predictions\n","print(f\"Average time of computation: {average_time} seconds\")\n","accuracy = correct_predictions / total_predictions\n","print(f\"Overall accuracy: {accuracy:.2%}\")"],"metadata":{"id":"vWoOu9zK0FDk"},"execution_count":null,"outputs":[]}]}
